[llm]# llm config
name='groq:llama-3.3-70b-versatile'
base_url=''
api_key="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"
max_tokens=1024
temperature=0.0 # The "temperature" parameter in a Large Language Model (LLM) controls the randomness and creativity of the generated text, with lower values resulting in more predictable and conservative outputs, and higher values leading to more diverse and creative outputs. 
prompt='''
    You are an AI assistant to answer questions based on a given context.

    Follow these guidelines:
    - ALWAYS use the knowledge base that is provided in the context between <context> </context> tags to answer user questions.
    - Never make assumptions or provide information not present in the knowledge base.
    - If information is not found in the knowledge base, politely acknowledge this.
'''

[embedder] # text embedding model config
model='sentence-transformers/all-MiniLM-L6-v2'
token="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"
normalize=true


[chunker] # chunker config
chunk_size=1024
chunk_overlap=128
#chunk_strategy="table_level" # "field_level"

[retriever] # retriever config
top_k=3

[reranker] # Text reranking model config
model="BAAI/bge-reranker-base"
token="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"
top_k=4

[file_paths]# Data files names and paths config
src_dir="D:/DataFiles"
#db_file="db/chinook.sqlite"
kb_dir="isro_pdfs"
vector_db_dir="vector_db"
bm_25_dir="bm_25"
eval_file="eval/intra_eval.json"
eval_file_with_response="eval/intra_eval_with_response.json"
results_dir="results"

[logfire]#logfire config
token="SGtFQXq0qRhrb6xKjhYt0pGkWSVHY395z3cv4X10jV6C"

[judge_llm]#Evaluator or Judge LLM config
name="gemma2-9b-it"
base_url=""
api_key="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"
max_tokens=1024
temparature=0.0
prompt=""


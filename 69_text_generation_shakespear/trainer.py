import torch
import torch.nn as nn
from b_model_basic import *
import torch.nn.functional as F
from torch.utils.data import DataLoader
import os
from matplotlib import  pyplot as plt
from matplotlib.ticker import MaxNLocator
from a_data_loader import *

class Trainer:
    # trainer doesnt inherit from any class...
    def __init__(self,model):
        self.model=model
        self.device=torch.device("cude" if torch.cuda.is_available() else "cpu")

    # here we are trying to calculate the overall loss....
    def evaluate_model(self,data_loader):
        total_loss=0.0
        for test_input,expected_output in data_loader:
            test_input,expected_output=test_input.to(self.device),expected_output.to(self.device) # here we are setting on what device the tensor has to run...
            actual_output=self.model(test_input) # actual output is also called as logits....
            loss=F.cross_entropy(actual_output.flatten(0,1),expected_output.flatten())
            total_loss+=loss.item() # convering a tensor to number....
        return total_loss
    

    def plot_losses(self,epochs_seen,train_losses,val_losses,plot_dir,plot_file):
        fig,ax1=plt.subplots(figsize=(10,10))
        ax1.plot(epochs_seen,train_losses,label='Training loss')
        ax1.plot(epochs_seen,val_losses,linestyle='-.',label='Validation loss')
        ax1.set_xlabel('Epochs')
        ax1.set_ylabel('Loss')
        ax1.legend(loc='upper right')
        ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
        fig.tight_layout()

        if not os.path.exists(plot_dir):
            os.makedirs(plot_dir)
        plt.savefig(os.path.join(plot_dir,plot_file))



    def train(self,optimizer,loss_fn,epochs,batch_size,train_shuffle,train_dataset,val_dataset,val_shuffle,eval_freq,model_dir,model_file,plot_dir,plot_file):
        # optimizer is the mechanism for iterating over the loss generated by model to figure out for which weights the model has less loss.
        print(">>:Begin creating dataloaders...")
        train_dataloader=DataLoader(train_dataset,batch_size=batch_size,shuffle=train_shuffle)

        train_tokens=0
        for input_tokens, expected_tokens in train_dataloader:
            train_tokens+=input_tokens.numel()
        print("Training tokens:",train_tokens)
        print("Total train batches:",len(train_dataloader))

        val_dataloader=DataLoader(val_dataset,batch_size=batch_size,shuffle=val_shuffle)
        val_tokens=0
        for v_input_tokens, v_expected_tokens in val_dataloader:
            val_tokens+=v_input_tokens.numel()
        print("Val tokens:",val_tokens)
        print("Total val batches:",len(val_dataloader))

        print(">> End of creating data loaders")

        print("Begin Training")
        self.model.to(self.device)
        self.model.train()
        steps=0
        train_losses,val_losses=[],[]
        for epoch in range(epochs):
            for inputs,expected_output in train_dataloader:
                inputs,expected_output=inputs.to(self.device),expected_output.to(self.device)

                # forward pass
                logits=self.model(inputs)
                loss=F.cross_entropy(logits.flatten(0,1),expected_output.flatten())

                # backward pass
                optimizer.zero_grad() # clear old gradients
                loss.backward() # computes new gradients
                optimizer.step() # updates model parameters using calcualted gradients

                steps+=1
                if steps%eval_freq==0:
                    train_loss=self.evaluate_model(train_dataloader)
                    val_loss=self.evaluate_model(val_dataloader)
                    train_losses.append(train_loss)
                    val_losses.append(val_loss)
                    print(f"epoch {epoch+1} (Step {steps:06d})")
                    print(f"Train loss {train_loss:.3f} ,  Val loss {val_loss:.3f}")
        print(" End of Training process...")

        print("Begin Plot creatoin")
        epochs_tensor=torch.linspace(0,epochs,len(train_losses))
        self.plot_losses(epochs_tensor,train_losses,val_losses,plot_dir,plot_file)
        print(">>> end plot creation")

        print("Beginning saving model")
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        torch.save(self.model,os.path.join(model_dir,model_file))

        print("Ending saving model")

if __name__=='__main__':
    config={'vocab_size':50257,'context_length':10,'embed_dim':128,'hidden_dim':64,'n_heads':4}
    model=TextGenerationModel4(config)
    trainer=Trainer(model)

    epochs=3
    batch_size=16
    optimizer=torch.optim.SGD(model.parameters(),lr=0.1)
    loss_fn=torch.nn.CrossEntropyLoss()
    src_dir=r'D:\DataFiles\text_generation\shakespear_random'
    train_dir=os.path.join(src_dir,'train')
    val_dir=os.path.join(src_dir,'val')
    train_dataset=CustomDataset(train_dir,config['context_length'])
    val_dataset=CustomDataset(val_dir,config['context_length'])
    model_dir=os.path.join(src_dir,'output')
    model_file="text_generator_1.pkl"
    plot_dir=os.path.join(src_dir,'plots')
    plot_file="text_generator_1.pdf"

    trainer.train(optimizer,loss_fn,epochs,batch_size,True,train_dataset,val_dataset,False,100,model_dir,model_file,plot_dir,plot_file)



    









